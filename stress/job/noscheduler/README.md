# TOSCA Component for stress job

This component introduces the concept of *jobs scheduling*.

[types.yaml](types.yaml) contains the TOSCA definition itself while the [playbooks](playbooks/) directory contains Ansible playbooks that implement the TOSCA node lifecycle.

## TOSCA definition

The TOSCA description contains a node type called `stress.job.noscheduler.nodes.StressJob` that allows to run
the `stress` command on a remote host for a given duration.

To keep things simple we will reuse an existing component that allow to run an process as a job and to monitor
its execution. This component is called `org.ystia.yorc.samples.job.noscheduler.SpawnJob` and is available on
the [Ystia samples repository](https://github.com/ystia/tosca-samples/tree/develop/org/ystia/yorc/samples/job/noscheduler/components)

So as you probably expected now the `stress.job.noscheduler.nodes.StressJob` component inherits from the
`org.ystia.yorc.samples.job.noscheduler.SpawnJob` component.

By doing such inheritance we just have to specify the `command` property to define the command and arguments to
run.

### Properties

This node type redefines the following properties of the `org.ystia.yorc.samples.job.noscheduler.SpawnJob`
node type:

* `command`: is the command and arguments to run

### Attributes

We do not define any attributes for this node type. The attributes are defined in the `org.ystia.yorc.samples.job.noscheduler.SpawnJob` node type.

### Operations

First lets introduce the lifecycle for jobs. This lifecycle is not normative, this is an extension to the TOSCA
specification defined by Alien4Cloud and Ystia.
This lifecycle is defined within a TOSCA interface named `tosca.interfaces.node.lifecycle.Runnable` and is composed of the following 3 operations:

* `submit`: this operation is called when the job is submitted, generally to a scheduler. When submitting a job
  to a scheduler we generally get a job id in return. `org.ystia.yorc.samples.job.noscheduler.SpawnJob` is a bit
  different as it does not interact with a scheduler but spawn a process using the `nohup` command. It then
  compose a job id from the pid of the spawned process.
* `run`: if the `submit` operation succeeds, then the orchestrator will the `run` operation regularly to check to
  check the job status (based on the job id) and potentially retrieve produced logs or outputs.
* `cancel`: this operation is used to cancel the job (by killing the process in this case). This operation is
  invoked only if workflow itself is cancelled.

Alien4Cloud automatically recognizes the `tosca.interfaces.node.lifecycle.Runnable` interface and builds a
workflow named `run` that contains the `submit` and `run` operations. Note that those operations are not
part of the install workflow generated by Alien4Cloud that will be run when deploying an application.

So, now we have a way to run the `stress` command on a remote host. But what if the `stress` command is not
available on the remote host? Fortunately we can combine and associate different TOSCA interfaces for a same
component.
By doing this we can implement a `create` operation from the standard TOSCA interface that will install the
`stress` command during the deployment of the application.

## Next steps

Now we have scanned all the components we need to implement our demonstration. We are now able to create a TOSCA
application that assembles these components. This can be done manually in Alien4Cloud using a WYSIWYG TOSCA
editor. But in this repository we have stored the resulting TOSCA application generated by Alien4Cloud.
This application is stored as a YAML file we call a Topology Template that could be used as a template to create
new applications in Alien4Cloud.

So, let's move on the final section of the first part of this tutorial the [Topology Template](../../../topologies/monitoring/README.md) component in order to
generate load on the compute system.
